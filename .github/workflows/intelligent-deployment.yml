name: Intelligent Deployment Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_deployment:
        description: 'Force deployment even with test failures'
        required: false
        default: 'false'
        type: boolean
      skip_performance_tests:
        description: 'Skip performance benchmarks'
        required: false
        default: 'false'
        type: boolean

env:
  NODE_VERSION: "18"
  PYTHON_VERSION: "3.11"
  PNPM_VERSION: "8.15.4"
  DEPLOYMENT_TIMEOUT: "1800" # 30 minutes
  TEST_TIMEOUT: "3600" # 1 hour

jobs:
  # =============================================================================
  # COMPREHENSIVE TEST VALIDATION PIPELINE
  # =============================================================================
  
  # Stage 1: Code Quality & Static Analysis
  code-quality-gate:
    name: Code Quality Gate
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      quality-passed: ${{ steps.quality-check.outputs.passed }}
      quality-score: ${{ steps.quality-check.outputs.score }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
          cache-dependency-path: 'frontend/pnpm-lock.yaml'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          # Backend dependencies
          pip install -r requirements.txt
          # Frontend dependencies  
          cd frontend && npm install -g pnpm@${{ env.PNPM_VERSION }} && pnpm install --frozen-lockfile

      - name: Backend Code Quality
        run: |
          echo "🔍 Running backend code quality checks..."
          make lint
          make type-check
          make security

      - name: Frontend Code Quality
        working-directory: ./frontend
        run: |
          echo "🔍 Running frontend code quality checks..."
          pnpm lint:ci
          pnpm type-check
          pnpm format:check

      - name: Quality Gate Decision
        id: quality-check
        run: |
          echo "📊 Evaluating code quality metrics..."
          python scripts/intelligent-quality-gate.py --stage=quality
          echo "passed=$(cat .quality-gate-result)" >> $GITHUB_OUTPUT
          echo "score=$(cat .quality-score)" >> $GITHUB_OUTPUT

      - name: Upload quality reports
        uses: actions/upload-artifact@v4
        with:
          name: quality-reports
          path: |
            quality-reports/
            frontend/reports/
        if: always()

  # Stage 2: Security Validation
  security-validation:
    name: Security Validation
    runs-on: ubuntu-latest
    needs: [code-quality-gate]
    timeout-minutes: 25
    outputs:
      security-passed: ${{ steps.security-check.outputs.passed }}
      vulnerabilities: ${{ steps.security-check.outputs.vulnerabilities }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup environments
        run: |
          pip install -r requirements.txt
          cd frontend && npm install -g pnpm@${{ env.PNPM_VERSION }} && pnpm install --frozen-lockfile

      - name: Backend Security Scan
        run: |
          echo "🔒 Running backend security scans..."
          make security-strict
          bandit -r . -f json -o security-reports/bandit-report.json || true
          safety check --json --output security-reports/safety-report.json || true

      - name: Frontend Security Scan
        working-directory: ./frontend
        run: |
          echo "🔒 Running frontend security scans..."
          pnpm audit --audit-level moderate
          pnpm test:security

      - name: Container Security Scan
        run: |
          echo "🐳 Scanning Docker images..."
          docker build -t dotmac-security-test .
          # Add container scanning here (e.g., Trivy)

      - name: Security Assessment
        id: security-check
        run: |
          python scripts/intelligent-quality-gate.py --stage=security
          echo "passed=$(cat .security-gate-result)" >> $GITHUB_OUTPUT
          echo "vulnerabilities=$(cat .vulnerability-count)" >> $GITHUB_OUTPUT

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: security-reports/
        if: always()

  # Stage 3: Unit Tests Matrix
  unit-tests-matrix:
    name: Unit Tests (${{ matrix.component }})
    runs-on: ubuntu-latest
    needs: [code-quality-gate]
    timeout-minutes: 20
    strategy:
      matrix:
        component: [
          'backend-core',
          'backend-services', 
          'frontend-admin',
          'frontend-customer',
          'frontend-reseller',
          'frontend-technician'
        ]
      fail-fast: false
    outputs:
      tests-passed: ${{ steps.test-results.outputs.passed }}
      coverage: ${{ steps.test-results.outputs.coverage }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        run: |
          if [[ "${{ matrix.component }}" == backend-* ]]; then
            pip install -r requirements.txt
          else
            cd frontend && npm install -g pnpm@${{ env.PNPM_VERSION }} && pnpm install --frozen-lockfile
          fi

      - name: Run unit tests
        run: |
          echo "🧪 Running unit tests for ${{ matrix.component }}..."
          python scripts/run-component-tests.py --component=${{ matrix.component }} --type=unit

      - name: Test Results Analysis
        id: test-results
        run: |
          python scripts/analyze-test-results.py --component=${{ matrix.component }}
          echo "passed=$(cat .test-result-${{ matrix.component }})" >> $GITHUB_OUTPUT
          echo "coverage=$(cat .coverage-${{ matrix.component }})" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-${{ matrix.component }}
          path: |
            test-results/
            coverage/
        if: always()

  # Stage 4: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests-matrix]
    timeout-minutes: 45
    outputs:
      integration-passed: ${{ steps.integration-results.outputs.passed }}
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      timescaledb:
        image: timescale/timescaledb:2.11.2-pg16
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_analytics_test
        ports:
          - 5433:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup full environment
        run: |
          pip install -r requirements.txt
          cd frontend && npm install -g pnpm@${{ env.PNPM_VERSION }} && pnpm install --frozen-lockfile

      - name: Initialize databases
        env:
          PGPASSWORD: test_password_123
        run: |
          echo "🗄️ Setting up test databases..."
          bash scripts/init-test-databases.sh

      - name: Backend Integration Tests
        env:
          DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost/dotmac_test
          REDIS_URL: redis://localhost:6379/0
          ANALYTICS_DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost:5433/dotmac_analytics_test
        run: |
          echo "🔗 Running backend integration tests..."
          make test-integration

      - name: Frontend Integration Tests
        working-directory: ./frontend
        env:
          REDIS_URL: redis://localhost:6379
        run: |
          echo "🔗 Running frontend integration tests..."
          pnpm test:integration

      - name: Cross-Portal Integration
        run: |
          echo "🌐 Testing cross-portal integration..."
          python scripts/test-portal-integration.py

      - name: Integration Results Analysis
        id: integration-results
        run: |
          python scripts/intelligent-quality-gate.py --stage=integration
          echo "passed=$(cat .integration-gate-result)" >> $GITHUB_OUTPUT

      - name: Upload integration results
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            integration-reports/
            test-results/
        if: always()

  # Stage 5: E2E Tests Across All 7 Portals
  e2e-portal-tests:
    name: E2E Tests (${{ matrix.portal }} on ${{ matrix.browser }})
    runs-on: ubuntu-latest
    needs: [integration-tests]
    timeout-minutes: 30
    strategy:
      matrix:
        portal: [
          'customer-portal',
          'admin-portal', 
          'reseller-portal',
          'technician-portal',
          'master-admin-portal',
          'tenant-admin-portal',
          'api-gateway-portal'
        ]
        browser: ['chromium', 'firefox', 'webkit']
      fail-fast: false
    outputs:
      e2e-passed: ${{ steps.e2e-results.outputs.passed }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'
          cache-dependency-path: 'frontend/pnpm-lock.yaml'

      - name: Install dependencies
        working-directory: ./frontend
        run: |
          pnpm install --frozen-lockfile
          pnpm playwright install --with-deps ${{ matrix.browser }}

      - name: Start test environment
        run: |
          echo "🚀 Starting test environment for ${{ matrix.portal }}..."
          docker-compose -f docker-compose.test.yml up -d
          bash scripts/wait-for-services.sh

      - name: Build and start portal
        run: |
          echo "🏗️ Building and starting ${{ matrix.portal }}..."
          python scripts/start-portal.py --portal=${{ matrix.portal }} --browser=${{ matrix.browser }}

      - name: Run E2E tests
        working-directory: ./frontend
        run: |
          echo "🎭 Running E2E tests for ${{ matrix.portal }} on ${{ matrix.browser }}..."
          pnpm test:e2e --project=${{ matrix.browser }} --grep="${{ matrix.portal }}"
        env:
          PLAYWRIGHT_BASE_URL: http://localhost:3000

      - name: E2E Results Analysis
        id: e2e-results
        run: |
          python scripts/analyze-e2e-results.py --portal=${{ matrix.portal }} --browser=${{ matrix.browser }}
          echo "passed=$(cat .e2e-result-${{ matrix.portal }}-${{ matrix.browser }})" >> $GITHUB_OUTPUT

      - name: Upload E2E results
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.portal }}-${{ matrix.browser }}
          path: |
            frontend/test-results/
            frontend/playwright-report/
        if: always()

      - name: Cleanup
        run: docker-compose -f docker-compose.test.yml down -v
        if: always()

  # Stage 6: Performance & Core Web Vitals
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: ${{ github.event.inputs.skip_performance_tests != 'true' }}
    timeout-minutes: 60
    outputs:
      performance-passed: ${{ steps.perf-results.outputs.passed }}
      core-vitals-score: ${{ steps.perf-results.outputs.vitals-score }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup performance test environment
        run: |
          pip install -r requirements.txt
          cd frontend && npm install -g pnpm@${{ env.PNPM_VERSION }} && pnpm install --frozen-lockfile
          docker-compose -f docker-compose.perf.yml up -d

      - name: Lighthouse Performance Audit
        working-directory: ./frontend
        run: |
          echo "🚨 Running Lighthouse performance audit..."
          pnpm build
          pnpm start &
          sleep 30
          npx lighthouse http://localhost:3000 --output=json --output-path=lighthouse-report.json

      - name: Core Web Vitals Testing
        working-directory: ./frontend
        run: |
          echo "⚡ Testing Core Web Vitals..."
          pnpm test:perf --grep="@performance"

      - name: Load Testing
        run: |
          echo "🏋️ Running load tests..."
          python scripts/run-load-tests.py --duration=300 --users=100

      - name: Performance Analysis
        id: perf-results
        run: |
          python scripts/analyze-performance-results.py
          echo "passed=$(cat .performance-gate-result)" >> $GITHUB_OUTPUT
          echo "vitals-score=$(cat .core-vitals-score)" >> $GITHUB_OUTPUT

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            performance-reports/
            frontend/lighthouse-report.json
        if: always()

  # Stage 7: Accessibility Testing
  accessibility-validation:
    name: Accessibility Validation
    runs-on: ubuntu-latest
    needs: [integration-tests]
    timeout-minutes: 25
    outputs:
      a11y-passed: ${{ steps.a11y-results.outputs.passed }}
      a11y-score: ${{ steps.a11y-results.outputs.score }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup accessibility testing
        working-directory: ./frontend
        run: |
          pnpm install --frozen-lockfile
          pnpm playwright install --with-deps chromium

      - name: Build application
        working-directory: ./frontend
        run: |
          pnpm build
          pnpm start &
          sleep 30

      - name: Axe Accessibility Testing
        working-directory: ./frontend
        run: |
          echo "♿ Running accessibility tests..."
          pnpm a11y:test
          pnpm test:a11y

      - name: Generate accessibility report
        working-directory: ./frontend
        run: |
          echo "📊 Generating accessibility report..."
          node scripts/a11y-test.js --output=reports/accessibility/

      - name: Accessibility Analysis
        id: a11y-results
        run: |
          python scripts/analyze-accessibility-results.py
          echo "passed=$(cat .a11y-gate-result)" >> $GITHUB_OUTPUT
          echo "score=$(cat .a11y-score)" >> $GITHUB_OUTPUT

      - name: Upload accessibility results
        uses: actions/upload-artifact@v4
        with:
          name: accessibility-results
          path: frontend/reports/accessibility/
        if: always()

  # =============================================================================
  # INTELLIGENT DECISION ENGINE
  # =============================================================================
  
  intelligent-decision-engine:
    name: Intelligent Decision Engine
    runs-on: ubuntu-latest
    needs: [
      code-quality-gate,
      security-validation,
      unit-tests-matrix,
      integration-tests,
      e2e-portal-tests,
      performance-benchmarks,
      accessibility-validation
    ]
    if: always()
    timeout-minutes: 10
    outputs:
      deployment-decision: ${{ steps.decision.outputs.action }}
      deployment-ready: ${{ steps.decision.outputs.ready }}
      failure-analysis: ${{ steps.decision.outputs.analysis }}
      remediation-plan: ${{ steps.decision.outputs.remediation }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/

      - name: Intelligent Analysis
        id: decision
        run: |
          echo "🧠 Running intelligent decision engine..."
          python scripts/intelligent-decision-engine.py \
            --quality-passed=${{ needs.code-quality-gate.outputs.quality-passed }} \
            --security-passed=${{ needs.security-validation.outputs.security-passed }} \
            --integration-passed=${{ needs.integration-tests.outputs.integration-passed }} \
            --performance-passed=${{ needs.performance-benchmarks.outputs.performance-passed || 'true' }} \
            --a11y-passed=${{ needs.accessibility-validation.outputs.a11y-passed }} \
            --force-deployment=${{ github.event.inputs.force_deployment || 'false' }}
          
          echo "action=$(cat .deployment-action)" >> $GITHUB_OUTPUT
          echo "ready=$(cat .deployment-ready)" >> $GITHUB_OUTPUT
          echo "analysis=$(cat .failure-analysis)" >> $GITHUB_OUTPUT
          echo "remediation=$(cat .remediation-plan)" >> $GITHUB_OUTPUT

      - name: Generate Decision Report
        run: |
          python scripts/generate-decision-report.py
          echo "📋 Decision report generated"

      - name: Upload decision reports
        uses: actions/upload-artifact@v4
        with:
          name: decision-reports
          path: |
            decision-reports/
            .deployment-*
            .failure-analysis
            .remediation-plan
        if: always()

  # =============================================================================
  # SERVER STARTUP AUTOMATION (When All Tests Pass)
  # =============================================================================
  
  production-deployment:
    name: Production Server Startup
    runs-on: ubuntu-latest
    needs: [intelligent-decision-engine]
    if: |
      needs.intelligent-decision-engine.outputs.deployment-ready == 'true' && 
      github.ref == 'refs/heads/main' &&
      github.event_name == 'push'
    timeout-minutes: 30
    environment: production
    outputs:
      deployment-status: ${{ steps.deploy.outputs.status }}
      server-urls: ${{ steps.deploy.outputs.urls }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup deployment environment
        run: |
          echo "🚀 Preparing production deployment..."
          pip install -r requirements.txt

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          path: deployment-artifacts/

      - name: Start Production Servers
        id: deploy
        run: |
          echo "🌟 Starting all 7 portal applications..."
          python scripts/production-server-startup.py \
            --config=deployments/production-config.yml \
            --timeout=${{ env.DEPLOYMENT_TIMEOUT }}
          
          echo "status=$(cat .deployment-status)" >> $GITHUB_OUTPUT
          echo "urls=$(cat .server-urls)" >> $GITHUB_OUTPUT

      - name: Initialize Load Balancer
        run: |
          echo "⚖️ Configuring load balancer and reverse proxy..."
          python scripts/configure-load-balancer.py

      - name: Setup SSL/TLS and Security Headers
        run: |
          echo "🔒 Configuring SSL certificates and security headers..."
          python scripts/configure-security.py

      - name: Initialize Monitoring
        run: |
          echo "📊 Setting up monitoring and alerting..."
          python scripts/setup-monitoring.py

      - name: Health Checks and Readiness Probes
        run: |
          echo "🩺 Configuring health checks..."
          python scripts/setup-health-checks.py

      - name: Deployment Verification
        run: |
          echo "✅ Verifying deployment..."
          python scripts/verify-deployment.py --timeout=300

      - name: Upload deployment logs
        uses: actions/upload-artifact@v4
        with:
          name: deployment-logs
          path: |
            deployment-logs/
            .deployment-status
            .server-urls
        if: always()

  # =============================================================================
  # ALIGNMENT CONTINUATION (When Tests Fail)
  # =============================================================================
  
  alignment-remediation:
    name: Alignment and Remediation
    runs-on: ubuntu-latest
    needs: [intelligent-decision-engine]
    if: |
      always() &&
      needs.intelligent-decision-engine.outputs.deployment-ready != 'true'
    timeout-minutes: 45
    outputs:
      remediation-status: ${{ steps.remediate.outputs.status }}
      fixes-applied: ${{ steps.remediate.outputs.fixes }}
      rerun-needed: ${{ steps.remediate.outputs.rerun }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/

      - name: Failure Analysis
        run: |
          echo "🔍 Analyzing test failures..."
          python scripts/analyze-failures.py \
            --artifacts-path=test-artifacts/ \
            --analysis-output=failure-analysis.json

      - name: Automated Remediation
        id: remediate
        run: |
          echo "🔧 Applying automated fixes..."
          python scripts/automated-remediation.py \
            --failures=failure-analysis.json \
            --max-fixes=10
          
          echo "status=$(cat .remediation-status)" >> $GITHUB_OUTPUT
          echo "fixes=$(cat .fixes-applied)" >> $GITHUB_OUTPUT
          echo "rerun=$(cat .rerun-needed)" >> $GITHUB_OUTPUT

      - name: Create Remediation Branch
        if: steps.remediate.outputs.fixes > 0
        run: |
          echo "🌿 Creating remediation branch..."
          git config user.name "Intelligent CI/CD Bot"
          git config user.email "cicd-bot@dotmac.framework"
          git checkout -b "auto-remediation/$(date +%Y%m%d-%H%M%S)"
          git add .
          git commit -m "🤖 Automated remediation fixes

          Applied fixes for:
          ${{ needs.intelligent-decision-engine.outputs.failure-analysis }}
          
          🧮 Generated with Intelligent CI/CD Pipeline
          
          Co-Authored-By: Intelligent-Bot <noreply@dotmac.framework>"
          git push origin HEAD

      - name: Generate Remediation Report
        run: |
          echo "📋 Generating remediation recommendations..."
          python scripts/generate-remediation-report.py

      - name: Upload remediation results
        uses: actions/upload-artifact@v4
        with:
          name: remediation-results
          path: |
            remediation-reports/
            failure-analysis.json
            .remediation-status
            .fixes-applied
        if: always()

  # =============================================================================
  # ITERATIVE TESTING (Re-run after remediation)
  # =============================================================================
  
  iterative-retest:
    name: Iterative Re-testing
    uses: ./.github/workflows/intelligent-deployment.yml
    needs: [alignment-remediation]
    if: |
      needs.alignment-remediation.outputs.rerun-needed == 'true' &&
      needs.alignment-remediation.outputs.fixes-applied > 0
    with:
      skip_performance_tests: true
    secrets: inherit

  # =============================================================================
  # MONITORING AND ALERTING
  # =============================================================================
  
  monitoring-setup:
    name: Setup Monitoring & Alerting
    runs-on: ubuntu-latest
    needs: [production-deployment]
    if: success()
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure Real-time Monitoring
        run: |
          echo "📊 Setting up real-time monitoring..."
          python scripts/setup-realtime-monitoring.py

      - name: Setup Performance Alerting
        run: |
          echo "⚠️ Configuring performance alerts..."
          python scripts/configure-alerts.py --type=performance

      - name: Security Monitoring
        run: |
          echo "🔒 Setting up security monitoring..."
          python scripts/setup-security-monitoring.py

      - name: User Experience Monitoring
        run: |
          echo "👥 Configuring UX monitoring..."
          python scripts/setup-ux-monitoring.py

  # =============================================================================
  # NOTIFICATIONS AND REPORTING
  # =============================================================================
  
  notification-center:
    name: Notification Center
    runs-on: ubuntu-latest
    needs: [
      intelligent-decision-engine,
      production-deployment,
      alignment-remediation,
      monitoring-setup
    ]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-artifacts/

      - name: Generate Comprehensive Report
        run: |
          echo "📊 Generating comprehensive pipeline report..."
          python scripts/generate-comprehensive-report.py

      - name: Success Notifications
        if: needs.production-deployment.result == 'success'
        run: |
          echo "✅ Sending success notifications..."
          python scripts/send-notifications.py --type=success \
            --deployment-status="${{ needs.production-deployment.outputs.deployment-status }}" \
            --server-urls="${{ needs.production-deployment.outputs.server-urls }}"

      - name: Failure Notifications  
        if: needs.intelligent-decision-engine.outputs.deployment-ready != 'true'
        run: |
          echo "❌ Sending failure notifications..."
          python scripts/send-notifications.py --type=failure \
            --analysis="${{ needs.intelligent-decision-engine.outputs.failure-analysis }}" \
            --remediation="${{ needs.intelligent-decision-engine.outputs.remediation-plan }}"

      - name: Progress Notifications
        if: needs.alignment-remediation.result == 'success'
        run: |
          echo "🔄 Sending progress notifications..."
          python scripts/send-notifications.py --type=progress \
            --fixes="${{ needs.alignment-remediation.outputs.fixes-applied }}" \
            --status="${{ needs.alignment-remediation.outputs.remediation-status }}"

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const deploymentReady = '${{ needs.intelligent-decision-engine.outputs.deployment-ready }}';
            const action = '${{ needs.intelligent-decision-engine.outputs.deployment-decision }}';
            
            let statusEmoji = deploymentReady === 'true' ? '✅' : '❌';
            let statusText = deploymentReady === 'true' ? 'PASSED - Ready for Production' : 'FAILED - Alignment Required';
            
            const commentBody = `## 🤖 Intelligent Deployment Pipeline Results
            
            ### Overall Status: ${statusEmoji} ${statusText}
            
            **Decision:** ${action}
            
            ### Test Results Summary
            | Stage | Status | Details |
            |-------|--------|---------|
            | Code Quality | ${{ needs.code-quality-gate.outputs.quality-passed == 'true' && '✅' || '❌' }} | Score: ${{ needs.code-quality-gate.outputs.quality-score || 'N/A' }} |
            | Security | ${{ needs.security-validation.outputs.security-passed == 'true' && '✅' || '❌' }} | Vulnerabilities: ${{ needs.security-validation.outputs.vulnerabilities || 'N/A' }} |
            | Integration | ${{ needs.integration-tests.outputs.integration-passed == 'true' && '✅' || '❌' }} | Cross-service validation |
            | E2E (7 Portals) | ${{ needs.e2e-portal-tests.outputs.e2e-passed == 'true' && '✅' || '❌' }} | All portal workflows |
            | Performance | ${{ needs.performance-benchmarks.outputs.performance-passed == 'true' && '✅' || '❌' }} | Core Web Vitals: ${{ needs.performance-benchmarks.outputs.core-vitals-score || 'N/A' }} |
            | Accessibility | ${{ needs.accessibility-validation.outputs.a11y-passed == 'true' && '✅' || '❌' }} | A11y Score: ${{ needs.accessibility-validation.outputs.a11y-score || 'N/A' }} |
            
            ${deploymentReady === 'true' ? `
            ### 🚀 Production Deployment
            **Status:** ${{ needs.production-deployment.outputs.deployment-status || 'In Progress' }}
            **Server URLs:** ${{ needs.production-deployment.outputs.server-urls || 'Configuring...' }}
            
            ✅ All 7 portals are now running in production!
            ` : `
            ### 🔧 Remediation Status
            **Analysis:** ${{ needs.intelligent-decision-engine.outputs.failure-analysis || 'Analyzing...' }}
            **Plan:** ${{ needs.intelligent-decision-engine.outputs.remediation-plan || 'Generating...' }}
            **Fixes Applied:** ${{ needs.alignment-remediation.outputs.fixes-applied || '0' }}
            
            🔄 Automatic remediation is in progress. The system will continue iterating until all tests pass.
            `}
            
            ### 📊 Detailed Reports
            - [📋 Decision Analysis](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [🔍 Test Coverage](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [⚡ Performance Metrics](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            - [♿ Accessibility Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ---
            🤖 *Generated by Intelligent CI/CD Pipeline - Auto-deploys on 100% test success*
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

      - name: Update GitHub Status
        run: |
          echo "🔄 Updating GitHub commit status..."
          python scripts/update-github-status.py \
            --status="${{ needs.intelligent-decision-engine.outputs.deployment-ready == 'true' && 'success' || 'failure' }}" \
            --context="Intelligent Deployment Pipeline" \
            --description="${{ needs.intelligent-decision-engine.outputs.deployment-decision }}"

      - name: Upload final reports
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-pipeline-report
          path: |
            pipeline-reports/
            all-artifacts/
        if: always()
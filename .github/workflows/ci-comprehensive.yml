name: Comprehensive CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 4 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      run_performance_tests:
        description: 'Run performance tests'
        required: false
        default: 'false'
      run_security_tests:
        description: 'Run security tests'
        required: false
        default: 'false'

env:
  PYTHON_VERSION: "3.11"

jobs:
  # Job 1: Code Quality Pipeline
  code-quality:
    name: Code Quality Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Run comprehensive linting
        run: |
          echo "::group::Code Quality Checks"
          make lint
          echo "::endgroup::"

      - name: Run type checking
        run: |
          echo "::group::Type Checking"
          make type-check
          echo "::endgroup::"
        continue-on-error: true

      - name: Run security scanning
        run: |
          echo "::group::Security Scanning"
          make security
          echo "::endgroup::"

      - name: Generate complexity report
        run: |
          make complexity-report
        continue-on-error: true

      - name: Upload code quality reports
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-reports
          path: |
            complexity-report.md
            bandit-report.json
            safety-report.json
        if: always()

  # Job 2: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [code-quality]
    
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Run unit tests
        run: |
          python -m pytest \
            -m "unit and not slow" \
            --tb=short \
            --cov-report=xml:coverage-unit-${{ matrix.python-version }}.xml \
            --cov-report=html:htmlcov-unit-${{ matrix.python-version }} \
            --junit-xml=junit-unit-${{ matrix.python-version }}.xml \
            -v

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: |
            junit-unit-${{ matrix.python-version }}.xml
            coverage-unit-${{ matrix.python-version }}.xml
            htmlcov-unit-${{ matrix.python-version }}/
        if: always()

  # Job 3: Integration Tests  
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests]
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      timescaledb:
        image: timescale/timescaledb:2.11.2-pg16
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_analytics_test
        ports:
          - 5433:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Set up test databases
        env:
          PGPASSWORD: test_password_123
        run: |
          # Create test databases
          psql -h localhost -U dotmac_test -d dotmac_test -c "
            CREATE DATABASE dotmac_identity_test;
            CREATE DATABASE dotmac_billing_test;
            CREATE DATABASE dotmac_services_test;
            CREATE DATABASE dotmac_networking_test;
            CREATE DATABASE dotmac_platform_test;
          "

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost/dotmac_test
          REDIS_URL: redis://localhost:6379/0
          ANALYTICS_DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost:5433/dotmac_analytics_test
          TESTING: true
        run: |
          python -m pytest \
            -m "integration and not slow" \
            --tb=short \
            --cov-report=xml:coverage-integration.xml \
            --cov-report=html:htmlcov-integration \
            --junit-xml=junit-integration.xml \
            -v

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            junit-integration.xml
            coverage-integration.xml
            htmlcov-integration/
        if: always()

  # Job 4: End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [integration-tests]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml --profile notifications --profile filestorage up -d --build

      - name: Wait for services to be ready
        run: |
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T postgres-test pg_isready -U dotmac_test; do sleep 5; done'
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T redis-test redis-cli ping; do sleep 5; done'

      - name: Run E2E tests
        run: |
          docker-compose -f docker-compose.test.yml exec -T test-runner \
            python -m pytest \
            -m "e2e" \
            --tb=short \
            --junit-xml=/app/test-reports/junit-e2e.xml \
            --html=/app/test-reports/e2e-report.html \
            --self-contained-html \
            -v

      - name: Copy test results
        run: |
          docker-compose -f docker-compose.test.yml cp test-runner:/app/test-reports ./e2e-test-reports

      - name: Stop test environment
        run: |
          docker-compose -f docker-compose.test.yml --profile notifications --profile filestorage down -v
        if: always()

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: e2e-test-reports/
        if: always()

  # Job 5: Performance Tests (Optional)
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [integration-tests]
    if: github.event.inputs.run_performance_tests == 'true' || github.event_name == 'schedule'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml --profile performance up -d --build

      - name: Wait for services
        run: |
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T postgres-test pg_isready -U dotmac_test; do sleep 5; done'
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T redis-test redis-cli ping; do sleep 5; done'

      - name: Run performance tests
        run: |
          docker-compose -f docker-compose.test.yml exec -T test-runner \
            python -m pytest \
            -m "performance" \
            --tb=short \
            --benchmark-json=/app/test-reports/benchmark-report.json \
            --junit-xml=/app/test-reports/junit-performance.xml \
            -v

      - name: Run load tests
        run: |
          docker-compose -f docker-compose.test.yml run --rm locust-test \
            --host=http://test-runner:8000 \
            --headless \
            --users=20 \
            --spawn-rate=2 \
            --run-time=60s \
            --html=/app/test-reports/locust-report.html \
            --csv=/app/test-reports/locust-stats

      - name: Copy performance results
        run: |
          docker-compose -f docker-compose.test.yml cp test-runner:/app/test-reports ./performance-test-reports

      - name: Stop test environment
        run: |
          docker-compose -f docker-compose.test.yml --profile notifications --profile filestorage down -v
        if: always()

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: performance-test-reports/
        if: always()

  # Job 6: Security Tests (Optional)
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [code-quality]
    if: github.event.inputs.run_security_tests == 'true' || github.event_name == 'schedule'
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Run security tests
        env:
          DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost/dotmac_test
          TESTING: true
        run: |
          python -m pytest \
            -m "security" \
            --tb=short \
            --junit-xml=junit-security.xml \
            -v

      - name: Upload security test results
        uses: actions/upload-artifact@v4
        with:
          name: security-test-results
          path: junit-security.xml
        if: always()

  # Job 7: Contract Tests
  contract-tests:
    name: API Contract Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [unit-tests]
    
    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: dotmac_test
          POSTGRES_PASSWORD: test_password_123
          POSTGRES_DB: dotmac_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.lock

      - name: Run contract tests
        env:
          DATABASE_URL: postgresql://dotmac_test:test_password_123@localhost/dotmac_test
          TESTING: true
        run: |
          python -m pytest \
            -m "contract" \
            --tb=short \
            --junit-xml=junit-contract.xml \
            -v

      - name: Upload contract test results
        uses: actions/upload-artifact@v4
        with:
          name: contract-test-results
          path: junit-contract.xml
        if: always()

  # Job 8: Quality Gate Check
  quality-gate:
    name: Quality Gate Check
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [code-quality, unit-tests, integration-tests, contract-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml

      - name: Download all test artifacts
        uses: actions/download-artifact@v4

      - name: Run quality gate check
        run: |
          python scripts/quality-gate-check.py --environment ${{ github.ref == 'refs/heads/main' && 'production' || 'development' }}

      - name: Upload quality gate report
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-report
          path: quality-reports/
        if: always()

  # Job 9: Test Report Generation
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [quality-gate]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install plotly pandas pyyaml

      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate comprehensive test report
        run: |
          python scripts/generate-test-report.py --output-dir comprehensive-test-reports

      - name: Upload comprehensive test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: comprehensive-test-reports/
        if: always()

      - name: Comment on PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Read quality gate results
            let qualityResults = '';
            try {
              const files = fs.readdirSync('quality-reports');
              const latestReport = files.filter(f => f.startsWith('quality-gate')).sort().pop();
              if (latestReport) {
                const reportData = JSON.parse(fs.readFileSync(`quality-reports/${latestReport}`, 'utf8'));
                
                qualityResults = `
            ## 🧪 Test Results Summary
            
            **Overall Status:** ${reportData.overall_passed ? '✅ PASSED' : '❌ FAILED'}
            **Quality Checks:** ${reportData.passed_checks}/${reportData.total_checks} passed
            
            ### Quality Gate Results
            ${Object.entries(reportData.results).map(([key, result]) => 
              `- ${result.passed ? '✅' : '❌'} **${key.toUpperCase()}**`
            ).join('\n')}
            
            ${reportData.blocking_failures.length > 0 ? `
            ### ⚠️ Blocking Issues
            ${reportData.blocking_failures.map(issue => `- ${issue}`).join('\n')}
            ` : ''}
            
            ${reportData.warning_issues.length > 0 ? `
            ### ⚠️ Warnings
            ${reportData.warning_issues.map(issue => `- ${issue}`).join('\n')}
            ` : ''}
            
            [📊 View detailed test report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
              }
            } catch (error) {
              qualityResults = '❌ Could not generate test results summary';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: qualityResults
            });

  # Job 10: Deployment (Production only)
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [quality-gate, test-report]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to production
        run: |
          echo "🚀 Deploying to production environment"
          echo "This is where you would deploy to your production infrastructure"
          echo "All quality gates have passed!"

  # Job 11: Notification
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [deploy, test-report]
    if: always()
    
    steps:
      - name: Notify on success
        if: needs.deploy.result == 'success' || (needs.test-report.result == 'success' && github.ref != 'refs/heads/main')
        run: |
          echo "✅ CI/CD Pipeline completed successfully!"
          # Add your notification logic here (Slack, email, etc.)

      - name: Notify on failure
        if: failure()
        run: |
          echo "❌ CI/CD Pipeline failed!"
          # Add your failure notification logic here
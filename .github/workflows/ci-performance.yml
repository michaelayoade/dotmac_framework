name: Performance Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '60'
      users:
        description: 'Number of concurrent users'
        required: false
        default: '10'

env:
  PYTHON_VERSION: "3.11"

jobs:
  performance-tests:
    name: Performance & Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Start test environment
        run: |
          echo "::group::Starting Performance Test Environment"
          docker-compose -f docker-compose.test.yml --profile performance up -d --build
          echo "::endgroup::"

      - name: Wait for services to be ready
        run: |
          echo "::group::Waiting for Services"
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T postgres-test pg_isready -U dotmac_test; do sleep 5; done'
          timeout 300 bash -c 'until docker-compose -f docker-compose.test.yml exec -T redis-test redis-cli ping; do sleep 5; done'
          
          # Wait for API to be ready
          timeout 300 bash -c 'until curl -f http://localhost:8000/health 2>/dev/null; do sleep 5; done' || true
          echo "Services are ready!"
          echo "::endgroup::"

      - name: Run performance benchmarks
        env:
          TEST_DURATION: ${{ github.event.inputs.duration || '60' }}
          TEST_USERS: ${{ github.event.inputs.users || '10' }}
        run: |
          echo "::group::Performance Benchmarks"
          docker-compose -f docker-compose.test.yml exec -T test-runner \
            python -m pytest \
            -m "performance" \
            --tb=short \
            --benchmark-json=/app/test-reports/benchmark-report.json \
            --junit-xml=/app/test-reports/junit-performance.xml \
            -v
          echo "::endgroup::"

      - name: Run load tests with Locust
        env:
          LOCUST_USERS: ${{ github.event.inputs.users || '10' }}
          LOCUST_RUN_TIME: ${{ github.event.inputs.duration || '60' }}s
        run: |
          echo "::group::Load Testing"
          docker-compose -f docker-compose.test.yml run --rm locust-test \
            --host=http://test-runner:8000 \
            --headless \
            --users=${LOCUST_USERS} \
            --spawn-rate=1 \
            --run-time=${LOCUST_RUN_TIME} \
            --html=/app/test-reports/locust-report.html \
            --csv=/app/test-reports/locust-stats
          echo "::endgroup::"

      - name: Copy test results
        run: |
          docker-compose -f docker-compose.test.yml cp test-runner:/app/test-reports ./performance-reports
          ls -la performance-reports/ || true

      - name: Analyze performance results
        run: |
          echo "::group::Performance Analysis"
          
          # Check if benchmark report exists
          if [ -f "performance-reports/benchmark-report.json" ]; then
            echo "## Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
            python3 -c "
import json
import sys

try:
    with open('performance-reports/benchmark-report.json', 'r') as f:
        data = json.load(f)
    
    print('| Test | Min (ms) | Max (ms) | Mean (ms) | Std Dev |')
    print('|------|----------|----------|-----------|---------|')
    
    for benchmark in data.get('benchmarks', []):
        name = benchmark.get('name', 'Unknown')
        stats = benchmark.get('stats', {})
        min_time = round(stats.get('min', 0) * 1000, 2)
        max_time = round(stats.get('max', 0) * 1000, 2) 
        mean_time = round(stats.get('mean', 0) * 1000, 2)
        std_dev = round(stats.get('stddev', 0) * 1000, 2)
        
        print(f'| {name} | {min_time} | {max_time} | {mean_time} | {std_dev} |')
        
except Exception as e:
    print(f'Error parsing benchmark results: {e}')
    sys.exit(0)
" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Check if locust report exists
          if [ -f "performance-reports/locust-stats_stats.csv" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Load Test Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python3 -c "
import csv
import sys

try:
    with open('performance-reports/locust-stats_stats.csv', 'r') as f:
        reader = csv.DictReader(f)
        
        print('| Endpoint | Requests | Failures | Median (ms) | 95%ile (ms) | RPS |')
        print('|----------|----------|----------|-------------|-------------|-----|')
        
        for row in reader:
            if row['Type'] == 'GET' or row['Type'] == 'POST':  # Filter out aggregated rows
                name = row.get('Name', 'Unknown')
                requests = row.get('Request Count', '0')
                failures = row.get('Failure Count', '0')
                median = row.get('Median Response Time', '0')
                p95 = row.get('95%ile Response Time', '0') 
                rps = row.get('Requests/s', '0')
                
                print(f'| {name} | {requests} | {failures} | {median} | {p95} | {rps} |')
                
except Exception as e:
    print(f'Error parsing locust results: {e}')
    sys.exit(0)
" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "::endgroup::"

      - name: Check performance thresholds
        run: |
          echo "::group::Performance Threshold Check"
          
          # Define performance thresholds
          MAX_RESPONSE_TIME_MS=2000
          MAX_ERROR_RATE=5.0
          MIN_THROUGHPUT_RPS=10
          
          threshold_failures=0
          
          # Check response time thresholds from locust results
          if [ -f "performance-reports/locust-stats_stats.csv" ]; then
            python3 -c "
import csv
import sys

MAX_RESPONSE_TIME_MS = $MAX_RESPONSE_TIME_MS
MAX_ERROR_RATE = $MAX_ERROR_RATE
MIN_THROUGHPUT_RPS = $MIN_THROUGHPUT_RPS

violations = []

try:
    with open('performance-reports/locust-stats_stats.csv', 'r') as f:
        reader = csv.DictReader(f)
        
        for row in reader:
            if row['Type'] in ['GET', 'POST']:
                name = row.get('Name', 'Unknown')
                median_time = float(row.get('Median Response Time', '0'))
                p95_time = float(row.get('95%ile Response Time', '0'))
                failure_count = int(row.get('Failure Count', '0'))
                request_count = int(row.get('Request Count', '1'))
                rps = float(row.get('Requests/s', '0'))
                
                error_rate = (failure_count / request_count) * 100 if request_count > 0 else 0
                
                if p95_time > MAX_RESPONSE_TIME_MS:
                    violations.append(f'❌ {name}: 95th percentile response time ({p95_time}ms) exceeds threshold ({MAX_RESPONSE_TIME_MS}ms)')
                
                if error_rate > MAX_ERROR_RATE:
                    violations.append(f'❌ {name}: Error rate ({error_rate:.2f}%) exceeds threshold ({MAX_ERROR_RATE}%)')
                
                if rps < MIN_THROUGHPUT_RPS and request_count > 0:
                    violations.append(f'❌ {name}: Throughput ({rps:.2f} RPS) below threshold ({MIN_THROUGHPUT_RPS} RPS)')

    if violations:
        print('Performance threshold violations:')
        for violation in violations:
            print(violation)
        sys.exit(1)
    else:
        print('✅ All performance thresholds met')
        
except Exception as e:
    print(f'Error checking thresholds: {e}')
    sys.exit(0)
"
          fi
          
          echo "::endgroup::"

      - name: Stop test environment
        run: |
          docker-compose -f docker-compose.test.yml down -v
        if: always()

      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-reports
          path: performance-reports/
        if: always()

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    needs: [performance-tests]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for comparison

      - name: Download current performance results
        uses: actions/download-artifact@v4
        with:
          name: performance-test-reports
          path: current-results/

      - name: Get baseline performance data
        run: |
          # Try to get performance data from main branch
          git checkout origin/main -- performance-reports/ 2>/dev/null || true
          
          if [ -d "performance-reports" ]; then
            mv performance-reports baseline-results/
          else
            echo "No baseline performance data found"
            mkdir -p baseline-results/
          fi

      - name: Compare performance
        run: |
          echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "baseline-results/benchmark-report.json" ] && [ -f "current-results/benchmark-report.json" ]; then
            python3 -c "
import json
import sys

try:
    # Load baseline results
    with open('baseline-results/benchmark-report.json', 'r') as f:
        baseline = json.load(f)
    
    # Load current results  
    with open('current-results/benchmark-report.json', 'r') as f:
        current = json.load(f)
    
    baseline_benchmarks = {b['name']: b for b in baseline.get('benchmarks', [])}
    current_benchmarks = {b['name']: b for b in current.get('benchmarks', [])}
    
    print('| Test | Baseline (ms) | Current (ms) | Change | Status |')
    print('|------|---------------|--------------|---------|---------|')
    
    for name in current_benchmarks:
        current_mean = current_benchmarks[name]['stats']['mean'] * 1000
        
        if name in baseline_benchmarks:
            baseline_mean = baseline_benchmarks[name]['stats']['mean'] * 1000
            change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100
            
            if change_pct > 20:
                status = '❌ Regression'
            elif change_pct > 10:
                status = '⚠️ Slower'
            elif change_pct < -10:
                status = '✅ Improved'
            else:
                status = '➡️ Similar'
            
            print(f'| {name} | {baseline_mean:.2f} | {current_mean:.2f} | {change_pct:+.1f}% | {status} |')
        else:
            print(f'| {name} | N/A | {current_mean:.2f} | New | ➕ New |')
            
except Exception as e:
    print(f'Error comparing results: {e}')
    sys.exit(0)
" >> $GITHUB_STEP_SUMMARY
          else
            echo "Unable to compare - missing benchmark data" >> $GITHUB_STEP_SUMMARY
          fi
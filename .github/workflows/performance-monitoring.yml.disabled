name: Performance Monitoring

on:
  schedule:
    # Run performance tests twice daily (6 AM and 6 PM UTC)
    - cron: "0 6,18 * * *"
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to test"
        required: true
        default: "staging"
        type: choice
        options:
          - staging
          - production
      test_type:
        description: "Type of performance test"
        required: false
        default: "full"
        type: choice
        options:
          - full
          - lighthouse
          - load
          - api
      duration:
        description: "Load test duration (minutes)"
        required: false
        default: "5"
        type: string

env:
  NODE_VERSION: "18.x"
  PNPM_VERSION: "8.x"
  ENVIRONMENT: ${{ github.event.inputs.environment || 'staging' }}
  TEST_DURATION: ${{ github.event.inputs.duration || '5' }}

jobs:
  # ===== LIGHTHOUSE PERFORMANCE =====
  lighthouse-performance:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'lighthouse' || github.event.inputs.test_type == 'full' || github.event_name == 'schedule'

    strategy:
      fail-fast: false
      matrix:
        portal:
          [
            admin,
            customer,
            reseller,
            technician,
            tenant-portal,
            management-admin,
            management-reseller,
          ]
        device: [desktop, mobile]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v3
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x

      - name: Get environment URL
        id: url
        run: |
          if [[ "${{ env.ENVIRONMENT }}" == "production" ]]; then
            echo "base_url=https://app.dotmac.com" >> $GITHUB_OUTPUT
          else
            echo "base_url=https://staging.dotmac.com" >> $GITHUB_OUTPUT
          fi

      - name: Run Lighthouse audit
        run: |
          lhci collect \
            --url="${{ steps.url.outputs.base_url }}/${{ matrix.portal }}" \
            --numberOfRuns=3 \
            --settings.chromeFlags="--no-sandbox --headless" \
            --settings.emulatedFormFactor="${{ matrix.device }}" \
            --settings.throttlingMethod=simulate

          lhci assert \
            --preset=lighthouse:recommended \
            --assertions.categories.performance=0.8 \
            --assertions.categories.accessibility=0.95 \
            --assertions.categories.best-practices=0.9 \
            --assertions.categories.seo=0.8
        env:
          LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}

      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-${{ matrix.portal }}-${{ matrix.device }}
          path: .lighthouseci/
          retention-days: 30

  # ===== API PERFORMANCE TESTING =====
  api-performance:
    name: API Performance Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'api' || github.event.inputs.test_type == 'full' || github.event_name == 'schedule'

    strategy:
      matrix:
        service: [isp-framework, management-platform]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Get service endpoint
        id: endpoint
        run: |
          if [[ "${{ env.ENVIRONMENT }}" == "production" ]]; then
            if [[ "${{ matrix.service }}" == "isp-framework" ]]; then
              echo "url=https://api.dotmac.com" >> $GITHUB_OUTPUT
            else
              echo "url=https://mgmt.dotmac.com" >> $GITHUB_OUTPUT
            fi
          else
            if [[ "${{ matrix.service }}" == "isp-framework" ]]; then
              echo "url=https://staging-api.dotmac.com" >> $GITHUB_OUTPUT
            else
              echo "url=https://staging-mgmt.dotmac.com" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Create API performance test
        run: |
          cat > api-performance-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend } from 'k6/metrics';

          const errorRate = new Rate('errors');
          const responseTime = new Trend('response_time');

          export let options = {
            stages: [
              { duration: '2m', target: 10 },   // Ramp up
              { duration: '${{ env.TEST_DURATION }}m', target: 50 },    // Load test
              { duration: '2m', target: 0 },    // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<2000'],  // 95% of requests under 2s
              http_req_failed: ['rate<0.01'],     // Error rate under 1%
              errors: ['rate<0.01'],
            },
          };

          const BASE_URL = '${{ steps.endpoint.outputs.url }}';

          export default function () {
            // Health check
            let healthRes = http.get(`${BASE_URL}/health`);
            check(healthRes, {
              'health check status is 200': (r) => r.status === 200,
              'health check response time < 500ms': (r) => r.timings.duration < 500,
            });
            errorRate.add(healthRes.status !== 200);
            responseTime.add(healthRes.timings.duration);

            // API status endpoint
            let statusRes = http.get(`${BASE_URL}/api/v1/status`);
            check(statusRes, {
              'status endpoint is 200': (r) => r.status === 200,
              'status response time < 1000ms': (r) => r.timings.duration < 1000,
            });
            errorRate.add(statusRes.status !== 200);
            responseTime.add(statusRes.timings.duration);

            // Metrics endpoint (if available)
            let metricsRes = http.get(`${BASE_URL}/metrics`);
            check(metricsRes, {
              'metrics endpoint response time < 2000ms': (r) => r.timings.duration < 2000,
            });

            sleep(1);
          }
          EOF

      - name: Run API performance test
        run: |
          k6 run --out json=api-performance-results.json api-performance-test.js

      - name: Upload API performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: api-performance-${{ matrix.service }}
          path: api-performance-results.json
          retention-days: 30

  # ===== LOAD TESTING =====
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'full' || github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Apache Bench
        run: sudo apt-get update && sudo apt-get install -y apache2-utils

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Set test targets
        id: targets
        run: |
          if [[ "${{ env.ENVIRONMENT }}" == "production" ]]; then
            echo "app_url=https://app.dotmac.com" >> $GITHUB_OUTPUT
            echo "api_url=https://api.dotmac.com" >> $GITHUB_OUTPUT
          else
            echo "app_url=https://staging.dotmac.com" >> $GITHUB_OUTPUT
            echo "api_url=https://staging-api.dotmac.com" >> $GITHUB_OUTPUT
          fi

      - name: Create comprehensive load test
        run: |
          cat > load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend, Counter } from 'k6/metrics';

          const errorRate = new Rate('errors');
          const responseTime = new Trend('response_time');
          const requests = new Counter('total_requests');

          export let options = {
            scenarios: {
              // Web application load
              web_load: {
                executor: 'ramping-vus',
                startVUs: 5,
                stages: [
                  { duration: '3m', target: 20 },
                  { duration: '${{ env.TEST_DURATION }}m', target: 50 },
                  { duration: '3m', target: 0 },
                ],
                exec: 'webTest',
              },
              // API load
              api_load: {
                executor: 'ramping-vus',
                startVUs: 10,
                stages: [
                  { duration: '3m', target: 30 },
                  { duration: '${{ env.TEST_DURATION }}m', target: 100 },
                  { duration: '3m', target: 0 },
                ],
                exec: 'apiTest',
              },
              // Spike test
              spike_test: {
                executor: 'ramping-vus',
                startTime: '2m',
                startVUs: 0,
                stages: [
                  { duration: '1m', target: 200 },
                  { duration: '1m', target: 0 },
                ],
                exec: 'apiTest',
              },
            },
            thresholds: {
              http_req_duration: ['p(95)<3000', 'p(99)<5000'],
              http_req_failed: ['rate<0.02'],
              errors: ['rate<0.02'],
            },
          };

          const WEB_URL = '${{ steps.targets.outputs.app_url }}';
          const API_URL = '${{ steps.targets.outputs.api_url }}';

          export function webTest() {
            let responses = http.batch([
              ['GET', `${WEB_URL}/admin`],
              ['GET', `${WEB_URL}/customer`],
              ['GET', `${API_URL}/health`],
            ]);

            for (let res of responses) {
              check(res, {
                'status is 200 or 302': (r) => r.status === 200 || r.status === 302,
                'response time < 3s': (r) => r.timings.duration < 3000,
              });
              errorRate.add(res.status >= 400);
              responseTime.add(res.timings.duration);
              requests.add(1);
            }

            sleep(Math.random() * 3 + 1);
          }

          export function apiTest() {
            let responses = http.batch([
              ['GET', `${API_URL}/health`],
              ['GET', `${API_URL}/api/v1/status`],
              ['GET', `${API_URL}/metrics`],
            ]);

            for (let res of responses) {
              check(res, {
                'API status is 200': (r) => r.status === 200,
                'API response time < 2s': (r) => r.timings.duration < 2000,
              });
              errorRate.add(res.status >= 400);
              responseTime.add(res.timings.duration);
              requests.add(1);
            }

            sleep(Math.random() * 2 + 0.5);
          }
          EOF

      - name: Run comprehensive load test
        run: |
          k6 run --out json=load-test-results.json load-test.js

      - name: Run Apache Bench concurrent test
        run: |
          # Test static resources
          ab -n 1000 -c 50 -g ab-results.tsv "${{ steps.targets.outputs.app_url }}/" > ab-results.txt 2>&1 || true

          # Test API endpoints
          ab -n 500 -c 25 "${{ steps.targets.outputs.api_url }}/health" > ab-api-results.txt 2>&1 || true

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-test-results.json
            ab-results.txt
            ab-api-results.txt
            ab-results.tsv
          retention-days: 30

  # ===== DATABASE PERFORMANCE =====
  database-performance:
    name: Database Performance Check
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'full' || github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install psycopg2-binary redis asyncio asyncpg aioredis

      - name: Create database performance test
        run: |
          cat > db_performance_test.py << 'EOF'
          import asyncio
          import asyncpg
          import aioredis
          import time
          import json
          import os
          from typing import Dict, List

          async def test_postgres_performance():
              """Test PostgreSQL performance"""
              results = {}

              # Connection test
              start_time = time.time()
              try:
                  conn = await asyncpg.connect(
                      host=os.getenv('DB_HOST', 'localhost'),
                      port=os.getenv('DB_PORT', 5432),
                      user=os.getenv('DB_USER', 'postgres'),
                      password=os.getenv('DB_PASSWORD', ''),
                      database=os.getenv('DB_NAME', 'dotmac')
                  )
                  connection_time = time.time() - start_time
                  results['connection_time'] = connection_time

                  # Simple query test
                  start_time = time.time()
                  await conn.fetchval('SELECT 1')
                  simple_query_time = time.time() - start_time
                  results['simple_query_time'] = simple_query_time

                  # Table count queries (if tables exist)
                  queries = [
                      "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'",
                      "SELECT version()",
                  ]

                  query_times = []
                  for query in queries:
                      start_time = time.time()
                      try:
                          await conn.fetchval(query)
                          query_time = time.time() - start_time
                          query_times.append(query_time)
                      except Exception as e:
                          query_times.append(-1)

                  results['avg_query_time'] = sum(q for q in query_times if q > 0) / len([q for q in query_times if q > 0]) if query_times else 0

                  await conn.close()

              except Exception as e:
                  results['error'] = str(e)
                  results['connection_time'] = -1

              return results

          async def test_redis_performance():
              """Test Redis performance"""
              results = {}

              start_time = time.time()
              try:
                  redis = aioredis.from_url(
                      os.getenv('REDIS_URL', 'redis://localhost:6379/0')
                  )

                  # Connection test
                  await redis.ping()
                  connection_time = time.time() - start_time
                  results['connection_time'] = connection_time

                  # SET/GET performance
                  start_time = time.time()
                  await redis.set('perf_test', 'test_value')
                  set_time = time.time() - start_time

                  start_time = time.time()
                  value = await redis.get('perf_test')
                  get_time = time.time() - start_time

                  results['set_time'] = set_time
                  results['get_time'] = get_time

                  # Cleanup
                  await redis.delete('perf_test')
                  await redis.close()

              except Exception as e:
                  results['error'] = str(e)
                  results['connection_time'] = -1

              return results

          async def main():
              print("Starting database performance tests...")

              postgres_results = await test_postgres_performance()
              redis_results = await test_redis_performance()

              report = {
                  'timestamp': time.time(),
                  'postgres': postgres_results,
                  'redis': redis_results,
                  'summary': {
                      'postgres_healthy': postgres_results.get('connection_time', -1) > 0,
                      'redis_healthy': redis_results.get('connection_time', -1) > 0,
                  }
              }

              with open('db-performance-results.json', 'w') as f:
                  json.dump(report, f, indent=2)

              print("Database performance test completed")
              print(f"PostgreSQL connection time: {postgres_results.get('connection_time', 'Failed')}")
              print(f"Redis connection time: {redis_results.get('connection_time', 'Failed')}")

          if __name__ == "__main__":
              asyncio.run(main())
          EOF

      - name: Run database performance test
        run: python db_performance_test.py
        env:
          DB_HOST: ${{ env.ENVIRONMENT == 'production' && secrets.PROD_DB_HOST || secrets.STAGING_DB_HOST }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_NAME: dotmac
          REDIS_URL: ${{ env.ENVIRONMENT == 'production' && secrets.PROD_REDIS_URL || secrets.STAGING_REDIS_URL }}

      - name: Upload database performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: database-performance
          path: db-performance-results.json
          retention-days: 30

  # ===== PERFORMANCE SUMMARY =====
  performance-summary:
    name: Performance Monitoring Summary
    runs-on: ubuntu-latest
    needs:
      [
        lighthouse-performance,
        api-performance,
        load-testing,
        database-performance,
      ]
    if: always()

    steps:
      - name: Download all performance results
        uses: actions/download-artifact@v4
        with:
          path: ./performance-results

      - name: Setup Node.js for analysis
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Analyze performance results
        run: |
          cat > analyze-performance.js << 'EOF'
          const fs = require('fs');
          const path = require('path');

          function analyzeResults() {
            const resultsDir = './performance-results';
            const summary = {
              timestamp: new Date().toISOString(),
              environment: process.env.ENVIRONMENT,
              lighthouse: {},
              api_performance: {},
              load_testing: {},
              database: {},
              alerts: []
            };

            // Analyze Lighthouse results
            const lighthouseDir = path.join(resultsDir);
            if (fs.existsSync(lighthouseDir)) {
              const files = fs.readdirSync(lighthouseDir, { recursive: true });
              const lighthouseFiles = files.filter(f => f.includes('lighthouse') && f.endsWith('.json'));

              console.log(`Found ${lighthouseFiles.length} Lighthouse result files`);
            }

            // Analyze API performance
            try {
              const apiFiles = fs.readdirSync(resultsDir).filter(f => f.includes('api-performance'));
              console.log(`Found ${apiFiles.length} API performance files`);
            } catch (e) {
              console.log('No API performance results found');
            }

            // Analyze load testing
            try {
              const loadFiles = fs.readdirSync(resultsDir).filter(f => f.includes('load-test'));
              console.log(`Found ${loadFiles.length} load test files`);
            } catch (e) {
              console.log('No load test results found');
            }

            // Generate alerts for performance issues
            if (summary.alerts.length > 0) {
              console.log(`⚠️  ${summary.alerts.length} performance alerts generated`);
            } else {
              console.log('✅ No performance issues detected');
            }

            // Write summary
            fs.writeFileSync('performance-summary.json', JSON.stringify(summary, null, 2));

            return summary;
          }

          const results = analyzeResults();
          console.log('Performance analysis completed');
          EOF

          node analyze-performance.js
        env:
          ENVIRONMENT: ${{ env.ENVIRONMENT }}

      - name: Generate performance report
        run: |
          echo "# Performance Monitoring Report" > performance-report.md
          echo "**Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> performance-report.md
          echo "**Environment:** ${{ env.ENVIRONMENT }}" >> performance-report.md
          echo "**Duration:** ${{ env.TEST_DURATION }} minutes" >> performance-report.md
          echo "" >> performance-report.md

          echo "## Test Results" >> performance-report.md
          echo "- **Lighthouse Performance:** ${{ needs.lighthouse-performance.result }}" >> performance-report.md
          echo "- **API Performance:** ${{ needs.api-performance.result }}" >> performance-report.md
          echo "- **Load Testing:** ${{ needs.load-testing.result }}" >> performance-report.md
          echo "- **Database Performance:** ${{ needs.database-performance.result }}" >> performance-report.md
          echo "" >> performance-report.md

          # Check for performance issues
          if [[ "${{ needs.lighthouse-performance.result }}" == "failure" ||
                "${{ needs.api-performance.result }}" == "failure" ||
                "${{ needs.load-testing.result }}" == "failure" ]]; then
            echo "⚠️ **Performance issues detected - review required**" >> performance-report.md
            echo "PERFORMANCE_STATUS=DEGRADED" >> $GITHUB_ENV
          else
            echo "✅ **All performance tests passed**" >> performance-report.md
            echo "PERFORMANCE_STATUS=HEALTHY" >> $GITHUB_ENV
          fi

      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary-${{ env.ENVIRONMENT }}
          path: |
            performance-summary.json
            performance-report.md
          retention-days: 90

      - name: Create performance issue (if degraded)
        if: env.PERFORMANCE_STATUS == 'DEGRADED'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');

            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `⚡ Performance Degradation Detected - ${process.env.ENVIRONMENT} - ${new Date().toISOString().split('T')[0]}`,
              body: `## Performance Monitoring Alert\n\n${report}\n\n**Environment:** ${process.env.ENVIRONMENT}\n**Action Required:** Please review performance metrics and optimize as needed.\n\n[View Performance Report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              labels: ['performance', 'monitoring', 'automated']
            });

      - name: Notify team of performance results
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          text: |
            ⚡ **Performance Monitoring Report**

            **Environment:** ${{ env.ENVIRONMENT }}
            **Status:** ${{ env.PERFORMANCE_STATUS == 'DEGRADED' && '⚠️ PERFORMANCE ISSUES' || '✅ HEALTHY' }}
            **Test Duration:** ${{ env.TEST_DURATION }} minutes

            **Results:**
            - Lighthouse: ${{ needs.lighthouse-performance.result }}
            - API Performance: ${{ needs.api-performance.result }}
            - Load Testing: ${{ needs.load-testing.result }}
            - Database: ${{ needs.database-performance.result }}

            [View Detailed Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

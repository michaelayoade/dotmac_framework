[tool:pytest]
testpaths = tests tests/generated tests/ai_generated
python_files = test_*.py *_test.py property_test_*.py contract_test_*.py behavior_test_*.py
python_classes = Test* Property* Contract* Behavior*
python_functions = test_* property_* contract_* behavior_*

# AI-First Configuration
addopts = 
    -v
    --tb=short
    --strict-markers
    --strict-config
    --maxfail=5
    --durations=10
    --dist=worksteal
    --numprocesses=auto
    # Smart coverage for AI-generated tests
    --cov=src/dotmac_isp
    --cov-report=term-missing:skip-covered
    --cov-fail-under=75
    --cov-branch
    --cov-config=.coveragerc-ai

asyncio_mode = auto
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

# AI-First Test Markers
markers =
    # Primary AI-First Test Types
    property_based: AI-generated property-based tests (40% of test suite)
    behavior: Business behavior and outcome tests (30% of test suite)
    contract: API and service contract validation tests (20% of test suite)
    smoke_critical: Revenue-critical path smoke tests only (10% of test suite)
    
    # Business-Critical Markers (DEPLOYMENT BLOCKERS)
    revenue_critical: Tests affecting billing, payments, or revenue (BLOCKS DEPLOYMENT ON FAILURE)
    billing_core: Core billing logic tests (BLOCKS DEPLOYMENT ON FAILURE)
    payment_flow: Payment processing workflow tests (BLOCKS DEPLOYMENT ON FAILURE)
    data_safety: Customer data protection and integrity tests (BLOCKS DEPLOYMENT ON FAILURE)
    customer_data_protection: GDPR/privacy compliance tests (BLOCKS DEPLOYMENT ON FAILURE)
    customer_journey: Complete customer workflow validation
    network_monitoring: Network device and infrastructure monitoring tests
    environment_validation: Test infrastructure and service validation
    integration: Cross-service integration testing
    
    # AI Safety Markers (DEPLOYMENT BLOCKERS) 
    ai_generated: Tests generated by AI systems
    ai_safety: Tests verifying AI-generated code safety (BLOCKS DEPLOYMENT ON FAILURE)
    business_logic_protection: Tests ensuring AI doesn't modify business rules (BLOCKS DEPLOYMENT ON FAILURE)
    
    # Performance and Monitoring
    performance_baseline: Tests that establish performance baselines for AI monitoring
    regression_detection: Tests that detect AI-introduced regressions
    
    # Legacy Markers (Optional)
    unit_legacy: Legacy unit tests (use sparingly)
    integration_legacy: Legacy integration tests (AI can generate better versions)
    slow_legacy: Legacy slow tests (convert to property-based)
    
    # Environment Markers
    database: Tests requiring database access
    external: Tests requiring external services
    docker: Tests running in Docker environment
    
    # Development Markers
    wip: Work in progress tests
    experimental: Experimental AI-generated tests
    startup_critical: Critical startup and initialization tests
    order: Test execution order markers
    asyncio: Async test markers from pytest-asyncio